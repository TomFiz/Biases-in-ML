{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Beyond Disparate Treatment & Disparate Impact : Learning Classification without Disparate Mistreatment\n",
    "\n",
    "This research paper presents a large-scale study on gender bias in occupation classification using machine learning. The study analyzes the impact of including explicit gender indicators in semantic representations of online biographies on occupation classification. The researchers created a new dataset of online biographies and used three different semantic representations (bag-of-words, word embeddings, and deep recurrent neural networks) to predict people's occupations. They considered two scenarios: one with explicit gender indicators and one without. The study found that including explicit gender indicators increased the true positive rates (TPR) for certain occupations associated with specific genders. This means that the occupation classifiers were more accurate in identifying the occupations of individuals when explicit gender indicators were present. However, even when these indicators were removed or \"scrubbed\" from the representations, significant TPR gender gaps remained. This indicates that gender bias still existed in the occupation classifiers, suggesting that the classifiers were more likely to correctly predict the occupation of individuals of one gender compared to individuals of the other gender. Furthermore, the study found that these TPR gender gaps were correlated with existing gender imbalances in occupations. This means that occupations with a greater gender imbalance were more likely to have larger TPR gender gaps. This suggests that occupation classifiers may compound existing gender imbalances by reinforcing the underrepresentation of certain genders in specific occupations. The researchers also analyzed proxy behavior that occurs in the absence of explicit gender indicators. They found that certain words, such as \"women,\" were highly predictive of gender even when gender indicators were removed. This indicates that gender information is not entirely removed by \"scrubbing\" explicit gender indicators, and that proxy behaviors can still perpetuate gender bias in occupation classification. Based on their findings, the study concludes that simply \"scrubbing\" explicit gender indicators is not sufficient to remove gender bias from occupation classifiers. More comprehensive approaches are needed to address gender bias, especially considering the existing gender imbalances in occupations. The study emphasizes the importance of designing and regulating occupation classifiers to prevent them from compounding gender imbalances and perpetuating gender bias. Overall, this research highlights the significance of addressing and mitigating bias in machine learning systems, particularly in high-stakes domains such as online recruiting and automated hiring. It demonstrates that fairness can be achieved without sacrificing performance, and provides insights into the complexities and challenges of combating gender bias in occupation classification.\n",
    "\n",
    "## Survey of Bias in Machine Learning Through the Prism of Statistical Parity\n",
    "\n",
    "We focused on a specific fairness criterion, the statistical parity, which is measured through the disparate impact\n",
    "Fairness is achieved when the algorithm behaves in the same way for both groups; when the sensitive variable does not play a significant role in the prediction\n",
    "It is made very clear from the outset that the aim of the article is to present a detailed analysis of the different approaches that have been proposed in the literature to study potential fairness issues in the binary classification problem through the adult income dataset\n",
    "We first point out that testing methods focus on individual fairness while statistical methods such as the Disparate Impact Analysis tackle the issue of group fairness\n",
    "As the rules considered are three of the best-known ML algorithms (LR, decision tree (DT), and GB) the explanations given on the proposed strategies have been brief and concise. We realize that this dataset can be considered as a toy example compared to the large databases that are generated and used nowadays by big and powerful multinationals, such as Amazon or Netflix mentioned in the introduction. It has been extensively studied in the literature on fairness in ML and we are well aware of the numerous solutions that have been proposed to solve this issue.\n",
    "We bring the following take-home messages: (i) Bias in the training data may lead to ML algorithms taking unfair decisions, but not always\n",
    "While, in this widely studied dataset, there was a clear increase of bias using the tested ML algorithms with respect to the gender variable, the ethnic origin did not lead in this case to a severe bias.\n",
    "For this reason, as mentioned above, we have focused in the rest of the article on the protected variable gender. These discriminations are more severe for the gender variable than for the ethnic origin variable.\n",
    "\n",
    "We proposed an ad hoc construction of confidence intervals for the disparate impact. (iii) Standard regulations that promote either the removal of the sensitive variable or the use of testing techniques appeared as irrelevant when dealing with fairness as statistical parity of ML algorithms.\n",
    "We believe this is crucial in order to guarantee a fair treatment for every subgroup of population, which will contribute to reduce the growing distrust of ML systems in the society."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
